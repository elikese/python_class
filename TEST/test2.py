import requests
from bs4 import BeautifulSoup
from docx import Document
import os
import re
import time


# ==========================================
# âš™ï¸ ì„¤ì •
# ==========================================

START_PAGE = 1  # ì‹œì‘ í˜ì´ì§€
END_PAGE = 2  # ë§ˆì§€ë§‰ í˜ì´ì§€
START_SEQ = 2975087  # 1í˜ì´ì§€ ì²« ê²Œì‹œê¸€ seq
POSTS_PER_PAGE = 10  # í˜ì´ì§€ë‹¹ ê²Œì‹œê¸€ ìˆ˜
SAVE_DIR = "output"  # ê²°ê³¼ ì €ì¥ í´ë”

BASE_URL = "https://overseas.mofa.go.kr/de-ko/brd/m_7204/view.do?seq={seq}&page={page}"

os.makedirs(SAVE_DIR, exist_ok=True)


# ==========================================
# ğŸ§© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==========================================


def clean_text(text: str) -> str:
    """ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°"""
    return re.sub(r"\s+", " ", text).strip()


def safe_filename(text: str) -> str:
    """íŒŒì¼ëª…ìœ¼ë¡œ ë¶€ì í•©í•œ ë¬¸ì ì œê±°"""
    return re.sub(r'[\\/*?:"<>|]', "", text)


# ==========================================
# ğŸ§  ê²Œì‹œê¸€ ìƒì„¸ ë°ì´í„° ì¶”ì¶œ
# ==========================================


def parse_post(seq: int, page: int):
    """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ì‘ì„±ì, ì‘ì„±ì¼, ë³¸ë¬¸ ì¶”ì¶œ"""
    url = BASE_URL.format(seq=seq, page=page)
    res = requests.get(url)
    if res.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url}")
        return None

    soup = BeautifulSoup(res.text, "lxml")

    # ì œëª©
    title_tag = soup.select_one(".bo_head h2")
    title = clean_text(title_tag.get_text()) if title_tag else f"ê²Œì‹œê¸€_{seq}"

    # ì‘ì„±ì / ì‘ì„±ì¼
    info_tags = soup.select(".bo_head dl dd")
    author = clean_text(info_tags[0].get_text()) if len(info_tags) > 0 else "ì‘ì„±ì ì—†ìŒ"
    date = clean_text(info_tags[1].get_text()) if len(info_tags) > 1 else "ë‚ ì§œ ì—†ìŒ"

    # ë³¸ë¬¸
    body_div = soup.find("div", class_="bo_con")
    if body_div:
        body_text = body_div.get_text(separator="\n", strip=True)
        body_text = re.sub(r"\s+", " ", body_text).strip()
    else:
        body_text = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "url": url,
        "title": title,
        "author": author,
        "date": date,
        "content": body_text,
    }


# ==========================================
# ğŸ’¾ DOCX íŒŒì¼ ì €ì¥
# ==========================================


def save_to_docx(post_data: dict):
    """ê²Œì‹œê¸€ì„ DOCX íŒŒì¼ë¡œ ì €ì¥"""
    document = Document()
    document.add_heading(post_data["title"], level=1)
    document.add_paragraph(f"URL: {post_data['url']}")
    document.add_paragraph(f"ì‘ì„±ì: {post_data['author']}")
    document.add_paragraph(f"ì‘ì„±ì¼: {post_data['date']}")
    document.add_paragraph("")  # ê³µë°± ì¤„
    document.add_paragraph(post_data["content"])

    safe_title = safe_filename(post_data["title"])
    safe_date = safe_filename(post_data["date"])
    filename = f"{safe_title}_{safe_date}.docx"
    path = os.path.join(SAVE_DIR, filename)

    document.save(path)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {path}")


# ==========================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==========================================


def main():
    print(f"ğŸ“„ {START_PAGE}í˜ì´ì§€ë¶€í„° {END_PAGE}í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§ ì‹œì‘...\n")

    for page in range(START_PAGE, END_PAGE + 1):
        print(f"ğŸ§­ {page}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")

        start_seq = START_SEQ - (page - 1) * POSTS_PER_PAGE
        for i in range(POSTS_PER_PAGE):
            seq = start_seq - i
            print(f"  â–¶ ê²Œì‹œê¸€ seq={seq} ìš”ì²­ ì¤‘...")

            try:
                post = parse_post(seq, page)
                if post:
                    save_to_docx(post)
                time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ (seq={seq}): {e}")

    print("\nğŸ‰ ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
