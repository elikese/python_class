import requests
from bs4 import BeautifulSoup
from docx import Document
import os
import re
import time


# ==========================================
# âš™ï¸ ì„¤ì • ì˜ì—­
# ==========================================

START_PAGE = 1  # ì‹œì‘ í˜ì´ì§€
END_PAGE = 2  # ë§ˆì§€ë§‰ í˜ì´ì§€
SAVE_DIR = "output"  # ê²°ê³¼ ì €ì¥ í´ë”

BASE_URL = "https://overseas.mofa.go.kr"
BOARD_PATH = "/de-ko/brd/m_7204"  # ë…ì¼ ëŒ€ì‚¬ê´€ ê²Œì‹œíŒ ê²½ë¡œ

LIST_URL = f"{BASE_URL}{BOARD_PATH}/list.do?page={{page}}"
VIEW_URL = f"{BASE_URL}{BOARD_PATH}/view.do?seq={{seq}}&page={{page}}"

os.makedirs(SAVE_DIR, exist_ok=True)


# ==========================================
# ğŸ§© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==========================================


def clean_text(text: str) -> str:
    """HTMLì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°"""
    return re.sub(r"\s+", " ", text).strip()


def safe_filename(text: str) -> str:
    """íŒŒì¼ëª…ìœ¼ë¡œ ë¶€ì í•©í•œ ë¬¸ì ì œê±°"""
    return re.sub(r'[\\/*?:"<>|]', "", text)


# ==========================================
# ğŸ“‹ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ í•¨ìˆ˜
# ==========================================


def get_post_links(page: int):
    """ì§€ì •ëœ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ seq ëª©ë¡ ì¶”ì¶œ"""
    url = LIST_URL.format(page=page)
    res = requests.get(url)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "lxml")

    post_links = []
    # ê²Œì‹œíŒ ëª©ë¡ì˜ ì œëª© ì˜ì—­ì—ì„œ ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ
    for a_tag in soup.select("table.board_list td.title a"):
        href = a_tag.get("href", "")
        m = re.search(r"seq=(\d+)", href)
        if m:
            seq = m.group(1)
            post_links.append(seq)

    # ì¤‘ë³µ ì œê±°
    post_links = list(dict.fromkeys(post_links))
    return post_links


# ==========================================
# ğŸ§  ê²Œì‹œê¸€ ìƒì„¸ ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
# ==========================================


def parse_post(seq: str, page: int):
    """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    url = VIEW_URL.format(seq=seq, page=page)
    res = requests.get(url)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "lxml")

    # ì œëª©
    title_tag = soup.select_one(".bo_head h2")
    title = clean_text(title_tag.get_text()) if title_tag else f"ê²Œì‹œê¸€_{seq}"

    # ì‘ì„±ì / ì‘ì„±ì¼
    info_tags = soup.select(".bo_head dl dd")
    author = clean_text(info_tags[0].get_text()) if len(info_tags) > 0 else "ì‘ì„±ì ì—†ìŒ"
    date = clean_text(info_tags[1].get_text()) if len(info_tags) > 1 else "ë‚ ì§œ ì—†ìŒ"

    # ë³¸ë¬¸
    content_tag = soup.select_one(".bo_con .se-contents")
    if content_tag:
        # <p> ë‹¨ìœ„ë¡œ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
        paragraphs = [clean_text(p.get_text()) for p in content_tag.find_all("p") if clean_text(p.get_text())]
        content = "\n".join(paragraphs)
    else:
        content = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "author": author,
        "date": date,
        "content": content,
    }


# ==========================================
# ğŸ’¾ DOCX íŒŒì¼ ì €ì¥ í•¨ìˆ˜
# ==========================================


def save_to_docx(post_data: dict):
    """ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ .docx íŒŒì¼ë¡œ ì €ì¥"""
    document = Document()
    document.add_heading(post_data["title"], level=1)
    document.add_paragraph(f"ì‘ì„±ì: {post_data['author']}")
    document.add_paragraph(f"ì‘ì„±ì¼: {post_data['date']}")
    document.add_paragraph("")  # ê³µë°± ì¤„
    document.add_paragraph(post_data["content"])

    safe_title = safe_filename(post_data["title"])
    safe_date = safe_filename(post_data["date"])
    filename = f"{safe_title}_{safe_date}.docx"
    path = os.path.join(SAVE_DIR, filename)

    document.save(path)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {path}")


# ==========================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==========================================


def main():
    print(f"ğŸ“„ {START_PAGE}í˜ì´ì§€ë¶€í„° {END_PAGE}í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§ ì‹œì‘...\n")

    for page in range(START_PAGE, END_PAGE + 1):
        print(f"ğŸ§­ {page}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")

        seq_list = get_post_links(page)

        if not seq_list:
            print(f"âš ï¸ {page}í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            continue

        for seq in seq_list:
            try:
                post = parse_post(seq, page)
                save_to_docx(post)
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ (seq={seq}): {e}")

    print("\nğŸ‰ ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
