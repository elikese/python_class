import os
import time
import re
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from docx import Document

# ===============================
# ğŸ§­ ì„¤ì • ì˜ì—­
# ===============================
BASE_URL = "https://overseas.mofa.go.kr/de-ko/brd/m_7204/view.do?seq={seq}&page={page}"

START_PAGE = 1  # ì‹œì‘ í˜ì´ì§€
END_PAGE = 2  # ë§ˆì§€ë§‰ í˜ì´ì§€
START_SEQ = 2975087  # 1í˜ì´ì§€ ì²« ê²Œì‹œê¸€ seq ê°’
POSTS_PER_PAGE = 10  # í˜ì´ì§€ë‹¹ ê²Œì‹œê¸€ ìˆ˜ (ê´€ì°° ê²°ê³¼ ì•½ 10ê°œ)

SAVE_DIR = "crawled_posts"  # ì €ì¥ í´ë”ëª…
os.makedirs(SAVE_DIR, exist_ok=True)


# ===============================
# ğŸ§° í•¨ìˆ˜ ì •ì˜
# ===============================
def fetch_html(url):
    """HTMLì„ ìš”ì²­í•˜ê³  BeautifulSoup ê°ì²´ë¡œ ë°˜í™˜"""
    headers = {"User-Agent": UserAgent().random}
    res = requests.get(url, headers=headers, timeout=10)
    res.raise_for_status()
    return BeautifulSoup(res.text, "lxml")


def parse_post(soup):
    """ê²Œì‹œê¸€ HTMLì—ì„œ ì œëª©, ì‘ì„±ì, ì‘ì„±ì¼, ë³¸ë¬¸ ì¶”ì¶œ"""
    title = soup.select_one("div.board_detail > div.bo_head > h2").get_text(strip=True)
    author = soup.select_one("div.board_detail > div.bo_head > dl > dd:nth-of-type(1)").get_text(strip=True)
    date = soup.select_one("div.board_detail > div.bo_head > dl > dd:nth-of-type(2)").get_text(strip=True)
    content_html = soup.select_one("div.board_detail > div.bo_con")
    content_text = content_html.get_text("\n", strip=True)
    return title, author, date, content_text


def save_to_docx(title, author, date, content):
    """ê²Œì‹œê¸€ì„ DOCX íŒŒì¼ë¡œ ì €ì¥"""
    safe_title = re.sub(r'[\\/:*?"<>|]', "_", title)  # íŒŒì¼ëª…ì—ì„œ ë¶ˆê°€ëŠ¥í•œ ë¬¸ì ì œê±°
    filename = f"{safe_title}_{date}.docx"
    filepath = os.path.join(SAVE_DIR, filename)

    doc = Document()
    doc.add_heading(title, level=1)
    doc.add_paragraph(f"ì‘ì„±ì: {author}")
    doc.add_paragraph(f"ì‘ì„±ì¼: {date}")
    doc.add_paragraph("")
    doc.add_paragraph(content)
    doc.save(filepath)
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")


# ===============================
# ğŸš€ í¬ë¡¤ë§ ì‹¤í–‰
# ===============================
if __name__ == "__main__":
    current_seq = START_SEQ

    for page in range(START_PAGE, END_PAGE + 1):
        print(f"\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")

        for i in range(POSTS_PER_PAGE):
            seq = current_seq - i - (page - START_PAGE) * POSTS_PER_PAGE
            url = BASE_URL.format(seq=seq, page=page)
            print(f"â†’ ìš”ì²­ ì¤‘: {url}")

            try:
                soup = fetch_html(url)
                title, author, date, content = parse_post(soup)
                save_to_docx(title, author, date, content)
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")

            time.sleep(1.5)  # ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€ìš© ë”œë ˆì´
